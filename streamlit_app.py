import openai
import streamlit as st
import requests
from bs4 import BeautifulSoup
import PyPDF2
import io
import pandas as pd
from typing import List, Dict
import json
import os
from datetime import datetime
from urllib.parse import urlparse
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class AdvancedLearningAI:
    def __init__(self, api_key: str):
        openai.api_key = api_key
        self.model = "gpt-3.5-turbo"
        self.messages = []
        self.knowledge_base = self.load_knowledge_base()
        self.vectorizer = TfidfVectorizer()
        self.vectors = None
        self.update_vectors()
    
    def load_knowledge_base(self) -> dict:
        """åŠ è½½çŸ¥è¯†åº“"""
        try:
            with open('advanced_knowledge_base.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {
                "web": {},
                "file": {},
                "manual": {}
            }
    
    def save_knowledge_base(self):
        """ä¿å­˜çŸ¥è¯†åº“"""
        with open('advanced_knowledge_base.json', 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)
        self.update_vectors()
    
    def update_vectors(self):
        """æ›´æ–°çŸ¥è¯†å‘é‡"""
        if not any(self.knowledge_base.values()):
            return
        
        all_content = []
        for category in self.knowledge_base.values():
            for item in category.values():
                all_content.append(item['content'])
        
        if all_content:
            self.vectors = self.vectorizer.fit_transform(all_content)
    
    def learn_from_web(self, url: str) -> str:
        """ä»ç½‘é¡µå­¦ä¹ """
        try:
            # éªŒè¯URL
            parsed_url = urlparse(url)
            if not parsed_url.scheme or not parsed_url.netloc:
                return "æ— æ•ˆçš„URL"
            
            # è·å–ç½‘é¡µå†…å®¹
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            # è§£æç½‘é¡µ
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–ä¸»è¦å†…å®¹
            main_content = ' '.join([p.text for p in soup.find_all(['p', 'article', 'section'])])
            title = soup.title.string if soup.title else url
            
            # ä¿å­˜åˆ°çŸ¥è¯†åº“
            self.knowledge_base['web'][url] = {
                'title': title,
                'content': main_content,
                'learned_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            self.save_knowledge_base()
            
            return f"æˆåŠŸå­¦ä¹ äº†ç½‘é¡µå†…å®¹ï¼š{title}"
            
        except Exception as e:
            return f"å­¦ä¹ ç½‘é¡µæ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
    
    def learn_from_file(self, file) -> str:
        """ä»æ–‡ä»¶å­¦ä¹ """
        try:
            file_type = file.type
            content = ""
            file_name = file.name
            
            if "pdf" in file_type.lower():
                pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.read()))
                content = " ".join([page.extract_text() for page in pdf_reader.pages])
            
            elif "text" in file_type.lower():
                content = file.getvalue().decode("utf-8")
            
            elif "excel" in file_type.lower() or "spreadsheet" in file_type:
                df = pd.read_excel(file)
                content = df.to_string()
            
            else:
                return "ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹"
            
            self.knowledge_base['file'][file_name] = {
                'content': content,
                'type': file_type,
                'learned_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            self.save_knowledge_base()
            
            return f"æˆåŠŸå­¦ä¹ äº†æ–‡ä»¶ï¼š{file_name}"
            
        except Exception as e:
            return f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
    
    def find_relevant_knowledge(self, query: str, top_k: int = 3) -> List[str]:
        """æ£€ç´¢ç›¸å…³çŸ¥è¯†"""
        if not self.vectors:
            return []
        
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.vectors).flatten()
        
        relevant_docs = []
        for idx in similarities.argsort()[-top_k:][::-1]:
            if similarities[idx] > 0.1:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                for category in self.knowledge_base.values():
                    for info in category.values():
                        if info['content'] == self.vectorizer.inverse_transform(self.vectors[idx])[0]:
                            relevant_docs.append(info['content'])
        
        return relevant_docs
    
    def generate_response(self, user_input: str) -> str:
        try:
            # è·å–ç›¸å…³çŸ¥è¯†
            relevant_knowledge = self.find_relevant_knowledge(user_input)
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å­¦ä¹ å’Œåˆ©ç”¨å„ç§æ¥æºçš„çŸ¥è¯†ã€‚"
                }
            ]
            
            if relevant_knowledge:
                knowledge_prompt = "æ ¹æ®ä»¥ä¸‹ç›¸å…³çŸ¥è¯†å›ç­”ï¼š\n" + "\n".join(relevant_knowledge)
                messages.append({"role": "system", "content": knowledge_prompt})
            
            messages.extend(self.messages)
            messages.append({"role": "user", "content": user_input})
            
            # ç”Ÿæˆå›ç­”
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
            
            ai_response = response.choices[0].message.content
            self.messages.append({"role": "assistant", "content": ai_response})
            
            return ai_response
            
        except Exception as e:
            return f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

def main():
    st.set_page_config(page_title="æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹", page_icon="ğŸ¤–", layout="wide")
    st.title("æ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ ğŸ¤–")
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "ai" not in st.session_state:
        api_key = os.getenv("OPENAI_API_KEY")
        st.session_state.ai = AdvancedLearningAI(api_key)
    
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # åˆ›å»ºä¸‰åˆ—å¸ƒå±€
    col1, col2, col3 = st.columns([2, 1, 1])
    
    # ä¸»å¯¹è¯åŒºåŸŸ
    with col1:
        st.subheader("å¯¹è¯åŒº")
        user_input = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")
        
        if user_input:
            with st.spinner("AIæ€è€ƒä¸­..."):
                response = st.session_state.ai.generate_response(user_input)
                st.session_state.chat_history.append({"role": "user", "content": user_input})
                st.session_state.chat_history.append({"role": "assistant", "content": response})
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        for msg in st.session_state.chat_history:
            if msg["role"] == "user":
                st.write(f"ğŸ‘¤ æ‚¨ï¼š{msg['content']}")
            else:
                st.write(f"ğŸ¤– AIï¼š{msg['content']}")
            st.write("---")
    
    # ç½‘é¡µå­¦ä¹ åŒºåŸŸ
    with col2:
        st.subheader("ä»ç½‘é¡µå­¦ä¹ ")
        url = st.text_input("è¾“å…¥ç½‘é¡µURLï¼š")
        if st.button("å­¦ä¹ ç½‘é¡µ"):
            with st.spinner("æ­£åœ¨å­¦ä¹ ç½‘é¡µå†…å®¹..."):
                result = st.session_state.ai.learn_from_web(url)
                st.write(result)
    
    # æ–‡ä»¶å­¦ä¹ åŒºåŸŸ
    with col3:
        st.subheader("ä»æ–‡ä»¶å­¦ä¹ ")
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ–‡ä»¶ï¼ˆæ”¯æŒPDFã€TXTã€Excelï¼‰",
            type=["pdf", "txt", "xlsx", "xls"]
        )
        if uploaded_file is not None:
            if st.button("å­¦ä¹ æ–‡ä»¶"):
                with st.spinner("æ­£åœ¨å­¦ä¹ æ–‡ä»¶å†…å®¹..."):
                    result = st.session_state.ai.learn_from_file(uploaded_file)
                    st.write(result)
    
    # çŸ¥è¯†åº“æ˜¾ç¤º
    st.sidebar.title("çŸ¥è¯†åº“")
    for category, items in st.session_state.ai.knowledge_base.items():
        if items:
            st.sidebar.subheader(f"{category.title()}çŸ¥è¯†")
            for source, info in items.items():
                with st.sidebar.expander(f"ğŸ“š {source}"):
                    st.write(f"å­¦ä¹ æ—¶é—´: {info['learned_at']}")
                    st.write(f"å†…å®¹é¢„è§ˆ: {info['content'][:200]}...")

if __name__ == "__main__":
    main()
